---
layout: post
title: Python Code Example | Importing CSVs into PostgreSQL at Scale
---

When I began my journey into data analytics/science I started out like many people - with my feet firmly planted in Excel and CSVs. Phew! I'm glad not to be limited to that anymore. 

SQL and Python are now the tools I use most, but that doesn't change the simple reality that many datasets will still be housed or shared in CSV format. That means you'll likely need to upload CSV files to a database management system (DBMS) like PostgreSQL (psql).

Uploading one or two CSVs to psql is simple enough - create a table and copy from your csv. However, if you have many CSVs or you don't want to constantly write/recall the SQL query, then you'll need to write a function to iteratively churn through CSVs and stuff them into your DBMS. I've written a function in Python to do just that. For me, this function has been very helpful, and it serves as a good example of my Python coding skills. 

    def CSVtoPostgreSQL(directory, database, user, host, port):
        
        tables_str = [filename.replace('.csv', '') for filename in os.listdir(directory)]
        
        for t in range(len(tables_str)):

            #Transfer CSV into Pandas
            the_csv = directory + '%s.csv' % tables_str[t]
            csv_in_pandas = pd.read_csv(the_csv)

            #Create 'CREATE TABLE query'
            fields = [str(x) for x in csv_in_pandas.columns]
            data_type = [str(csv_in_pandas[col].dtype) for col in [str(x) for x in csv_in_pandas.columns]]
            data_type_edited = [x for x in np.where(np.array(data_type) == 'int64', 'integer', data_type)]
            data_type_edited = [x for x in np.where(np.array(data_type_edited) == 'float64', 'numeric', data_type_edited)]
            data_type_edited = [x for x in np.where(np.array(data_type_edited) == 'object', 'text', data_type_edited)]
            zipped = zip(fields, data_type_edited)
            better_zip = [zipped[i][0] + ' ' + zipped[i][1] for i in range(len(zipped))]
            zip_clean = str(better_zip).replace('[', '(').replace(']', ')').replace('\'', '')
            create_table_query = 'create table %s%s;' % (tables_str[t], zip_clean)
            print 'SQL QUERY TO CREATE TABLE: ', create_table_query

            #Create 'COPY CSV query'
            table = tables_str[t]
            fill_fields = str(fields).replace('\'', '').replace('[', '(').replace(']', ')')
            copy_csv_query = 'copy %s%s from \'%s%s.csv\' delimiter \',\' csv header encoding \'utf-8\';' % (table, fill_fields, directory, table)
            print 'SQL QUERY TO INSERT CSV INTO POSTGRESQL: ', copy_csv_query
            print 
            
            #Connect to PostgreSQL and run queries 
            conn = psycopg2.connect(database='outbrain', user='allenbyron', host='localhost', port=5432)
            cur = conn.cursor()

            cur.execute(create_table_query)
            conn.commit()

            cur.execute(copy_csv_query)
            conn.commit()

            cur.close()
            conn.close()
        
As you can see, the function accesses a directory with n number of CSVs and uploads them to PostgreSQL. It won't handle unquoted carriage return, which I've since accomplished successfully. I won't delve into that as I don't want to ruin the magic :) If you have any questions about the code, or anything else, please feel free to get in touch. 