
When I begin my journey to become a data scientist I started out like many people - with my feet firmly planted in Excel and CSVs. Phew! I'm glad not to be limited to that. To me, data munging is best done in SQL (and even more so in Python's Pandas). However, there is a simple reality that many datasets will still be transfered in CSV files. That means if you want to munge away in SQL you'll need to upload CSV files to a database management system (DBMS) like PostgreSQL (psql).

Uploading one or two CSVs to psql is simple enough - create a table and copy from your csv. However, if you want to streamline this, because you have many CSVs like me or you don't want to constantly write/recall the SQL query, then you'll need to write a function to iteratively churn through CSVs and stuff them into your DBMS. I've done just that below.

I realised the usefulness of this and also wanted to use the opportunity to showcase some of my Python coding skills. 

'''
def CSVtoPostgreSQL(directory, database, user, host, port):
    
    tables_str = [filename.replace('.csv', '') for filename in os.listdir(directory)]
    
    for t in range(len(tables_str)):

        #Transfer CSV into Pandas
        the_csv = directory + '%s.csv' % tables_str[t]
        csv_in_pandas = pd.read_csv(the_csv)

        #Create 'CREATE TABLE query'
        fields = [str(x) for x in csv_in_pandas.columns]
        data_type = [str(csv_in_pandas[col].dtype) for col in [str(x) for x in csv_in_pandas.columns]]
        data_type_edited = [x for x in np.where(np.array(data_type) == 'int64', 'integer', data_type)]
        data_type_edited = [x for x in np.where(np.array(data_type_edited) == 'float64', 'numeric', data_type_edited)]
        data_type_edited = [x for x in np.where(np.array(data_type_edited) == 'object', 'text', data_type_edited)]
        zipped = zip(fields, data_type_edited)
        better_zip = [zipped[i][0] + ' ' + zipped[i][1] for i in range(len(zipped))]
        zip_clean = str(better_zip).replace('[', '(').replace(']', ')').replace('\'', '')
        create_table_query = 'create table %s%s;' % (tables_str[t], zip_clean)
        print 'SQL QUERY TO CREATE TABLE: ', create_table_query

        #Create 'COPY CSV query'
        table = tables_str[t]
        fill_fields = str(fields).replace('\'', '').replace('[', '(').replace(']', ')')
        directory = '/Users/allenbyron/Desktop/DS/KaggleCompetitions/outbrain_comp/data/'
        copy_csv_query = 'copy %s%s from \'%s%s.csv\' delimiter \',\' csv header encoding \'utf-8\';' % (table, fill_fields, directory, table)
        print 'SQL QUERY TO INSERT CSV INTO POSTGRESQL: ', copy_csv_query
        print 
        
        #Connect to PostgreSQL and run queries 
        conn = psycopg2.connect(database='outbrain', user='allenbyron', host='localhost', port=5432)
        cur = conn.cursor()

        cur.execute(create_table_query)
        conn.commit()

        cur.execute(copy_csv_query)
        conn.commit()

        cur.close()
        conn.close()
        '''

As you can see the function accesses a directory with n number of CSVs and uploads them to PostgreSQL. 

I won't go through the code line by line, but if you have any quesitons please let me know.